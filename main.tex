% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage[a4paper, total={6in, 8in}]{geometry}

\usepackage{mystyle}

\usepackage{natbib}
\bibliographystyle{abbrvnat}
\usepackage{hyperref}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}


\title{Implicit Differentiation}

\author{Justin Chiu \\
  Cornell Tech \\
  \texttt{jtc257@cornell.edu}}

\begin{document}
\maketitle
\begin{abstract}
Gradient-based learning forms the foundation of modern machine learning,
and automatic differentiation allows ML practitioners to easily
compute the gradients necessary for learning.
While automatic differentiation only costs a constant multiple of the time and space
required to evaluate a function, it has its limitations.
In particular, when evaluating a function itself is expensive,
the direct application of automatic differentiation is infeasible.
In this report, we review the implicit function theorem (IFT)
and its use in reducing the cost of computing gradients in scenarios where
function evaluation is expensive,
focusing on the application of implicit differentiation to variational inference.
\end{abstract}

\section{Introduction}
Gradient-based learning underpins many of the recent successes in machine learning,
particularly advances involving neural networks.
The key to the success of gradient-based methods is automatic differentiation (AD),
which has greatly increased the development speed of machine learning research by
allowing practitioners to circumvent the error-prone and time-consuming process
of computing gradients manually.
AD operates by reducing functions into compositions of atomic operations,
for which we have a library of derivatives for,
and composing those derivatives via the chain rule.
(introduce the representation of functions as evaluation procedures / computational graphs,
following \citet{griewank2008autodiff})
While efficient relative to the evaluation of the function in question,
taking only a multiplicative constant longer than the evaluation itself,
this may be prohibitively expensive if the original function evaluation itself is costly.
An example of this is if the function takes the form of an unrolled loop,
a common artifact of iterative methods.
As naive AD requires storing all of the intermediate values at each point,
storing the output of all computations at every iteration of a loop can quickly
become infeasible due to memory limitations.

There are a variety of methods for overcoming the space limitations of AD,
of which we only mention three: checkpointing, reversible computation, and implicit differentiation.
A first method, checkpointing, improves space complexity at the cost of time.
Rather than storing all intermediate computation,
checkpointing instead recomputes values when needed.
This can result in a large slowdown,
and also requires careful choosing of which computationals subgraphs to checkpoint.
A second method is an improvement upon checkpointing, called reversible computation \citep{maclaurin2015reversible,gomez2017reversible},
which improves space complexity at the cost of expressivity, but not speed.
Reversible computation ensures that the gradient with respect to input depends only on the output,
allowing the input to be discarded during function evaluation.
This is typically accomplished by ensuring that the input is easily reconstructed from the output,
restricting the expressivity of layers.
A third method is implicit differentiation,
which improves space complexity at the cost of {\color{red}BLANK}.
Implicit differentiation relies on the implicit function theorem (IFT) \citep{},
which gives conditions under which derivatives can be computed independent of
intermediate computation.
In this report, we will cover the implicit function theorem (IFT),
discuss the intuition behind implicit differentiation,
and apply it to reducing the computational cost of taking derivatives through variational inference.

\paragraph{Bilevel Optimization}
(Need to be careful and double check this sentence, since IFT is probably more general than just bilevel opt)
Informally, applications of implicit differentiation can commonly be formulated as bilevel optimization problems,
where, for every iteration when solving an outer optimization problem,
we must additionally solve an inner optimization problem.
Some examples of bilevel optimization problems are hyperparameter optimization, metalearning,
and variational inference.

Hyperparameter optimization formulates hyperparameter tuning, such as the shrinkage penalty in LASSO,
as a bilevel optimization problem by computing gradients wrt the penalty through the entire learning procedure
of the linear model \citep{lorraine2019implasso}.
(Other works on hyperparam opt \citep{maclaurin2015reversible,bertrand2020implicit})
Similarly, metalearning learns the parameters of a model such that is the model is able to quickly
be adapted to a new task via gradient descent \citep{finn2017maml,rajeswaran2019impmaml}.
This is accomplished by differentiating through the learning procedure of each new task.
Finally, a variant of variational inference follows a very similar format:
semi-amortized variational inference (SAVI) aims to learn a model that is able to initialize
variational parameters \citep{kim2018savi}.
This is also accomplished by differentiating through the iterative optimization procedure
applied to the variational parameters during inference.
(Other VI papers \citep{vi,johnson2017pgm})

There is also work on expressing individual layers of a neural network declaratively
as the solution of an optimization problem \citep{optnet,agrawal2019diffcvx,gould2019declarative}.
This also falls under the umbrella of bilevel optimization, as we have both the outer training loop
for the whole model and the inner optimization loop for each optimization layer.
(reword)

\section{The Implicit Function Theorem}

% Entries for the entire Anthology, followed by custom entries
\bibliography{bib}
%\bibliographystyle{natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

Neural ODEs use reversibility.

\end{document}
